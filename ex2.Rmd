---
title: "Ex 2 PS2"
output: pdf_document
date: "2025-05-04"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Exercise 2
The dataset glass contains data on n = 214 single glass fragments. Each case has a measured refractive
index (RI) and composition (weight percent of oxides of Na, Mg, Al, Si, K, Ca, Ba and Fe). The composition
sums to around 100%; what is not anything else is sand. The fragments are classified as six types (variable
type). The classes are window float glass (WinF), window non float glass (WinNF), vehicle window glass (Veh),
containers (Con), tableware (Tabl) and vehicle headlamps (Head).
```{r warning=FALSE, echo= FALSE, include=FALSE}
library(tidyverse)
library(MASS)
library(ellipse)
library(nnet)
library(class)
library(mclust)
```




```{r cars}
glass<-read.table("data/glass.txt",header=T)
glass$type<-factor(glass$type)
levels(glass$type)<-c("WinF","WinNF","Veh","Con","Tabl","Head")
table(glass$type)
```


```{r}
dim(glass)
```

```{r}
head(glass)
```

The matrix *glass* has 214 rows and 10 columns, the 9 predictors are the the following

* RI: the refractive index, which is adimensional
* 8 Chemical elements, all express as percentages of oxides of: 

  * Na: Sodium
  * Mg: Magnesium
  * Al: Aluminium
  * Si: Silicon
  * K: Potassium
  * Ca: Calcium
  * Ba: Barium
  * Fe: Iron
  
* Type : is the type of glass, which is the  __target variable__. Which has 7 possible values.

  1. building_windows_float_processed
  2. building_windows_non_float_processed
  3. vehicle_windows_float_processed
  4. vehicle_windows_non_float_processed 
  5. containers
  6. tableware
  7. headlamps

The target variable is of course analysized as a factor is -r.
```{r}
glass %>% is.na() %>% sum()
```
Furthermore we check that there are no NAs in the dataset.

```{r}
table(glass$type) %>%t() %>% apply(1,function(x) x/sum(x)) %>% t()
```
Before going on to the exercise we still need to do a little exploratory analysis of the data set. We start by plotting the relative frequency of each glass type
```{r, include=FALSE, echo =F}
lookup<-c("black", "blue", "brown", "gray60",
"green3", "orange")
names(lookup)<-as.character(unique(glass$type))
```

```{r}
#bar plot freq relative
glass %>%
  ggplot(aes(x = type, y = after_stat(count)/sum(after_stat(count)), fill = type)) + 
  geom_bar() +
  labs(title = "Relative Frequency of Classes",y = "Freq Rel")+
  scale_color_manual(values = lookup, aesthetics = "fill", name = "Class")+
  theme_minimal()
```
Such graph carries the same information of the table printed before but makes much clearer that some classes like tableware and containers are under represented in the data set, of course for any discrimination function it will be harder to recognize those classes when other classes are much more prevalent. In fact we notice that the first two classes are about 65% of the data, such situation is generally referred to as unbalanced data (i.e. When some classes have significantly more observation than others), but since no class of glass is more important than others we do not apply any oversampling or undersampling techinque. 

### Point 1
Use linear discriminant analysis to predict the glass type. Look at the first two discriminant directions:
what are the most important variables in separating the classes? Comment.

Before applying the LDA, we check wether the assumption of the model is verified, namely we check that each variable conditioned on class is normally distributed (i.e. each variable is a mixture of Gaussians) and if the  $\Sigma_i = \Sigma\quad \forall i \in(1,\dots,6)$ where $\Sigma_i$ is the variance structure of each group. We do this trough a violin plot which is a graph that estimates the density in a  non-parametric way (this is to check the normality assumption), and also plots the boxplot (this is to check that the variance assumption).
```{r}
glass %>%
  gather(key = Measure, value = value, -type )  %>%
  ggplot(aes(x = type, y= value, color = 
               type)) +
  scale_color_manual(values = lookup, name = "Class")+
  geom_violin(trim = T)+ 
  geom_boxplot(width=0.1) + 
  facet_wrap( ~ Measure ,scales = "free", ncol =3)+
  labs(title = "Violin Plot Conditioned on Classes") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We can see that none of the above conditons is respected, so we can say that our classifier wont archive very good results (to improve performance we could do box-cox transformation to render the variables more normal-like, but we will stick with the exercise outline). Since univariate normality is not verified it is useless to check multivariate normality.

We proceed by fitting the model:
```{r}
lda.fit<-lda(glass$type ~ . ,data=glass)
lda.fit
```

The coefficients like this are not interpretable since the variables have different unit of measurement, to get of such interpenetration we use the following formula $a^{*}=[\text{diag}(\hat{\Sigma})]^{1/2}a$. This formula scales vector of the coefficents based on the variance of each column

```{r}
diag(diag(cov(glass[,1:9])** (1/2))) %*% lda.fit$scaling 
```
LD1 rimane influenzata da Na-Si-Al-Ca (tutti negativi->valori bassi=LD1 alto)
LD2 rimane fortemente influenzata da Mg-Ca-Na-Si (tutti positivi->valori bassi=LD2 basso)

(interpretetion of this two things to me is completly senseless)

### Point 2
Compute the training error. Are there any groups less homogeneous than the others? Comment.

Let's start by computing the traning error:
```{r}
lda.pred  <-predict(lda.fit,glass)
training_error <- mean(lda.pred$class != glass$type )
training_error
```
In isolation this number is pretty meaningless since we don't know how much our model over-fitted the data.

We can check how homogeneous the groups are by seeing if our linear predictor predicts equally well all of the classes. We do that by printing the confusion matrix with both the absolute value:
```{r}
lda.pred  <-predict(lda.fit,glass)
conf.mat<-table(predicted=lda.pred$class,true=glass$type)
conf.mat2<-addmargins(conf.mat)
conf.mat2
```

And with the relative values:
```{r}
conf.mat<-apply(conf.mat,2,function(x) x/sum(x))
conf.mat
```
```{r}
diag(conf.mat)
```

From these table it is clear that the most heterogeneous class are the Head,WinF, WinF which are not so coincidentally the most common kinds of glass.
Meanwhile the other 3 are the least homogeneous with Veh being the least since every observation belonging to that group has been classified somewhere else.
Since we don't have any more data is difficult to understand whether more observation could improve significantly the result or their heterogeneity is due to basic characteristics of the materials.

  
### Point 3
Implement a 10-fold cross validation using the partition of the observations provided by the variable groupCV to estimate the error rate.Comment

We import the variable cv index and we insert it in the dataframe.
```{r}
cv.index <- read.csv("data/groupCV.txt",header = F)
glass$cv.index <- cv.index[,]
```
In this way the cross validation which is usually random becomes a deterministic procedure.

```{r}
errors <- rep(NA,10)
for (i in (1:10)){
  training_set <- glass[cv.index != i, ]  
  test_set <- glass[cv.index == i, ]  
  lda.fit.cv <-lda(training_set$type ~ . , data= training_set[,1:10])
  lda.pred.cv <- predict(lda.fit.cv,newdata = test_set[,1:10])
  errors[i]<-mean(lda.pred.cv$class != test_set$type )
}
mean(errors)
```
Using the CV we get a more accurate view of the error rate of our classifier by allowing our test set to be different for each iteration, in this way we leverage the law of large numbers. Of course the CV error rate is higher than the training error rate because models tend to overfit the training data. This difference is not huge so we can say that our model managed to generalize.


### Point 4
Use the first two discriminant variables for a two-dimensional representation of the data together with centroids by using color-coding for the 6 classes of the class variable type (use lookup color vector
below). Comment in view of the answer to point 2


```{r}
lookup<-c("black", "blue", "brown", "gray60",
"green3", "orange")
names(lookup)<-as.character(unique(glass$type))
lookup
```



```{r}
data.col<-lookup[glass$type]
means.hat.z<-aggregate(lda.pred$x,by=list(glass$type),FUN=mean)
#plot(LD2~LD1, 
     #data=lda.pred$x[,1:2],pch=16,col=data.col)
#points(means.hat.z[,1],means.hat.z[,2],pch=21,bg=lookup,cex=1.5, lwd=2, col="red")

LDscores <-tibble(
  LD1 = lda.pred$x[,1],
  LD2 = lda.pred$x[,2],
  TYPE = glass$type,
)

LDscores %>%
  ggplot(mapping=aes(x = LD1, y = LD2,col = TYPE))+
  geom_point( alpha  =0.5)+
  scale_color_manual(values = data.col, name = "Class")+
  geom_point(data = tibble(means.hat.z)[,1:3],
    aes(x= LD1, y= LD2, colour = Group.1),           
    shape  = 16,                         
    size   = 6,
    stroke = 1.4 
  )+
  labs(title = "Scatterplot of Linear Discriminant") +
  theme_minimal()

```
The bigger points represent the centroids for each group, that is the mean of the first two LDs for each class.
The scatter plot clearly highlights the patterns previously observed in the confusion matrix, emphasizing which classes are more isolated, which are less so, and which ones are more likely to be confused with others. In particular, the orange class, corresponding to the Head type of glass, appears to be the most distinctly separated. Conversely, the brown class, Veh, is the less homogenous, as it is positioned between the WinF and WinNF classes.
We also observe that the blue and grey classes, Con and WinNF, are easily confounded, as reflected in the confusion matrix, where 5 out of 13 Con instances were misclassified as WinNF.

### Point 5

Compute the training error and the 10-fold cross validation error for each reduced-rank LDA classifier.
Plot both error curves against the number of discriminant directions, add full-rank LDA errors found in
points 2. and 3. What classifier do you prefer? Comment.

We calculate the CV-errors and training error for each reduced classifier. What we expect is that the training error is monotonically non increasing, since an over-fitted model will always outperform in the training set. Meanwhile it is possible that the most complex wont perform the best since simpler model can generalize better.

```{r}
errors.train.reduced<- rep(NA,5)
errors.test.reduced<- rep(NA,5)

for (j in (1:5)){
  errors.test<- rep(NA,10)

  for (i in (1:10)){
    training_set <- glass %>% filter(cv.index != i) 
    test_set <- glass %>% filter(cv.index == i) 
    
    lda.fit.cv <-lda(training_set$type ~ .  , data= training_set[,1:10])
    lda.pred.test.cv <- predict(lda.fit.cv,test_set[,1:10], dim = j )
    errors.test[i]<-mean(lda.pred.test.cv$class != test_set$type )

  }
  
  lda.pred<-predict(lda.fit,glass,dimen=j)
  errors.train.reduced[j]<-mean(lda.pred$class!=glass$type)
  errors.test.reduced[j]<-mean(errors.test)
}
errors.test.reduced
errors.train.reduced
```

Now we plot the result side to side:
```{r}
df <- data.frame(
  iter  = rep(seq_along(errors.train.reduced), 2),
  error = c(errors.train.reduced, errors.test.reduced),
  serie = rep(c("Errors train", "Errors test"), each = length(errors.train.reduced))
)

ggplot(df, aes(iter, error, colour = serie)) +
  geom_line(linewidth = 1, linetype= 2) +
  geom_point(size   = 4)+
  labs(x = "Number of LDA", y = "Error",
       title = "Comparison Error Curves") +
  theme_bw()

```
As expected the Errors in the training set are monotonically non increasing and they are below the error in the test set.
Further is clear the we prefer classifier with 4 LDA since it has the lowest CV-error.


### Point 6


We use the support vector machine, we do so because is often considered the best out of the box model.
```{r}
library(e1071) 
library(caTools)
library(ggplot2)
library(class)

errors <- rep(NA,10)
errorsKNN <- rep(NA,10)

for (i in (1:10)){
  training_set <- glass %>% filter(cv.index != i)  %>% dplyr::select(-cv.index)
  test_set <- glass %>% filter(cv.index == i)  %>%dplyr::select(-cv.index)
  mod<-svm(training_set$type ~ . ,data=training_set)
  errors[i]<-mean(predict(mod,test_set) == test_set$type )
 
  mu<- colMeans(training_set[1:9])
  va<- diag(cov(training_set[1:9]))
  training_scale <- cbind( training_set[1:9] %>% scale() , training_set[10]) 
  test_scale<- cbind(test_set[1:9] %>% scale(center= mu, scale =va) , test_set[10])
  test_pred <- knn(
                 train = training_set[1:9], 
                 test = test_set[1:9],
                 cl = training_scale$type, 
                 k=2
                 )
  errorsKNN[i]<-mean(test_pred == test_scale$type )
}
mean(errors)
mean(errorsKNN)
```

As we can see the CV-error rate drops signlificantly with this method.



