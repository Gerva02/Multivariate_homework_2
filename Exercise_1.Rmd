---
title: "Exercise_1"
author: "Alice"
date: "2025-05-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
The data set pulp_paper contains measurements of properties of pulp fibers and 
the paper made from them. There are n = 62 observations on 4 paper properties: 
breaking length (BL), elastic modulus (EM), stress at failure (SF), burst 
strength (BS); and 4 pulp fiber characteristics: arithmetic fiber length (AFL), 
long fiber fraction (LFF), fine fiber fraction (FFF), zero span tensile (ZST).

```{r}
setwd("/Users/alice/Desktop/data/")
pulp_paper<-read.table("pulp_paper.txt",header=T)
names(pulp_paper)<- c("BL" ,"EM" ,"SF" ,"BS","AFL", "LFF", "FFF", "ZST")
dim(pulp_paper)
```
```{r}
head(pulp_paper)
```
Let's explain what the variables represent:

BL – Breaking Length: the length at which the paper breaks, a measure of its tensile strength.

EM – Elastic Modulus: the elastic modulus, indicates how stiff the paper is.

SF – Stress at Failure: the stress at which the paper fails, another measure of strength.

BS – Burst Strength: the pressure required to rupture the paper.

AFL – Arithmetic Fiber Length: the average fiber length.

LFF – Long Fiber Fraction: the fraction of long fibers in the sample.

FFF – Fine Fiber Fraction: the fraction of fine fibers.

ZST – Zero Span Tensile: a measure of the internal strength of the fibers, independent of their length.

### Point 1

Obtain the maximum likelihood solution for m = 2 and m = 3 common factors on the standardize
observations and compute the proportion of total sample variance due to each factor. List the estimated
communalities, specific variances, and the residual matrix $S-(\hat{L}\hat{L}^T+\Psi)$. Compare the results. Which choice of m do you prefer? Why?


To proceed with the Factor analysis using the Maximum Likelihood method we need that the normality assumption of the data must be satisfied.   
We investigate about it using the Mahalobis distance. 
```{r}
# Calcolo le  distanze di Mahalanobis
pulp_paper_scaled <- scale(pulp_paper)
p<- ncol(pulp_paper_scaled)
center <- colMeans(pulp_paper_scaled)
cov <- cov(pulp_paper_scaled)
mah <- mahalanobis(pulp_paper_scaled, center, cov)
chi2_quantiles <- qchisq(ppoints(pulp_paper_scaled), df=p)
# Q-Q plot
ggplot(cbind(chi2_quantiles, mah), aes(x = chi2_quantiles, y = mah)) +
geom_point( color = "blue", size = 1) +
geom_abline(slope = 1, intercept = 0, color = "red") +
theme_minimal() +
labs(title = "Q-Q plot of the Malahnobis Distances", x = "Chi-squared quantiles",
y = "Observed Mahalanobis Distances")
```
As we can see the initial points lie approximately on the reference line, suggesting that most of the data follow a multivariate normal distribution.
However, some points, particularly in the upper tail, deviate noticeably from the line, suggesting the presence of potential multivariate outliers or a deviation from normality. In the latter case, the use of the Maximum Likelihood method may not be appropriate, and the Principal Component Method might be a better choice.  
Anyways following the text we apply the ML method to our data.

At first we want to visualize the correlation matrix of the data pulp_paper to observe whether any underlying groupings become apparent. 
```{r}
S <- cor(pulp_paper)
round(S,3)
```
As we can see from the correlation matrix almost all the variables are positive correlated except for the variable FFF which is negative correlated with all the others variables.
Some variable show a very high correlation -like BL, EM, SF, BS and AFL, LFF, ZST- within each other and low correlation between the other ones that heuristically make sense because they are the properties related to the paper and the properties related to the fiber.
This makes us think that there could be two factors, however although the correlation within each group of variables (those associated with the same factor) is high, the correlation between the other variables outside the group are not so low to suggest us the independence of two dimension, so we need to investigate further with factor analysis.

So in order to proceed with the factor analysis we fix m=2 and apply the command
factanal(), which uses the maximum likelihood method, to the standardize data.  
We decide to use the rotation to have a better interpretation of the loadings. 
```{r}
fa.ml<-factanal(x=pulp_paper_scaled, factors=2, rotation="varimax")
fa.ml
```
We extract the loadings ($l_{jk}$ where $j$ represents the number of variables so $1 \leq j \leq 8$, while $k$ represents the number of factors so $k=1,2$) in order to say something about the Factors.
```{r}
names(fa.ml)
L<-fa.ml$loadings[,]
round(L,3)
```
What can we say about Factor1 and Factor2? 
As we supposed above using the correlation matrix Factor1 is related to the strength of the paper, in fact it shows high loadings for BL, EM, SF, and BS, which are all variables related to paper properties. Therefore, we can interpret Factor 1 as representing the strength or robustness of the paper.  
Factor2, on the other hand, has high loadings for AFL, LFF, and FFF, which are all related to fiber dimensions, suggesting that Factor2 reflects fiber length.  
The loadings related to variable ZST are similar on both factors, indicating that it is influenced by both paper strength and fiber length.

Now we extract the other elements: the uniqueness, the communalities and the residual matrix. 

The uniquenesses quantities which are contained in the factanal() represent 
$var(\epsilon_j)$ where $\epsilon_j$ represent the random errors (specific factors).
From theory we have that $var(\epsilon_j)=\psi_j$ which is a diagonal matrix. 
```{r}
round(fa.ml$uniquenesses,3)
Psi<-diag(fa.ml$uniquenesses)
```
As we can see the only factor with high specific variance is FFF, this means that this
variable in not well explained by the extration of the two factors. 


We extract the communalities of every variable which are calculate using the loadings, in particular using the formula: $h_j^2=l_{j1}^2+l_{j2}^2+\ldots+l_{jm}^2$, and check that the sum of uniqueness and the communalitites is 1 (as seen in theory for standardized variables)
```{r}
comm<-diag(crossprod(t(fa.ml$loadings)))
comm
round(fa.ml$uniquenesses+diag(crossprod(t(fa.ml$loadings))),3)
```
Now we want to find the residual, at first we define the matrix $\hat{S}=L^TL+\Psi$ which 
give us the correlation matrix derived from the method we have applied, and then we calculate
what is the error that we made, how much the estimated values differ from the real ones (how much the model fit our data). 
```{r}
S.hat<-L%*%t(L)+Psi; 
round(S.hat,3)
Residual<-S-S.hat
round(Residual,3)
res <- sum(Residual^2)
res
```
As we can see the error rate is 0.1064641 which is very low, so the model fit quite well our data. 

About the proportion of the sample variance we can see it directly from factanal() command or we can compute it  using the loadings. 
```{r}
p <- dim(pulp_paper)[2]
cum <- sum(diag(crossprod(fa.ml$loadings))/p*100)
prop_var <- round(c(diag(crossprod(fa.ml$loadings))/p*100, cum),1)
names(prop_var) <- c("Prop of var of Factor1", "Prop of var of Factor2", 
                     "Cumulative prop of var")
prop_var
```
There is not a big difference between the proportion of variance explained by the first and second factors.  
Anyway the two factors together explain the 88% of the variance which is a good proportion, so m=2 is a good candidate for the factor analysis.

Now repeat all of this using m=3.
```{r}
fa.ml_new<-factanal(x=pulp_paper_scaled, factors=3, rotation = "varimax")
fa.ml_new
```
Loadings
```{r}
names(fa.ml_new)
L_new<-fa.ml_new$loadings[,]
round(L_new,3)
```
Factor 1 is consistent with the previous analysis and represents the strength of the paper. 
Factor 2 now has high loadings only for AFL, LFF, FFF and ZST suggesting it represents fiber length. 
Factor 3 shows relatively “high” loadings only for the variables EM, FFF, and ZST, but these values do not exceed the commonly accepted threshold of 0.5/0.6, and therefore cannot be considered significant for interpretation purposes. As a result, we are unable to provide a meaningful interpretation for this factor. 


Specific variances
```{r}
names(fa.ml_new)
round(fa.ml_new$uniquenesses,3)
Psi_new<-diag(fa.ml_new$uniquenesses)
```
As above the only bigger value between the other is FFF which is less then the value obtained for m=2, but still quite big.

Communalities
```{r}
comm_new<-diag(crossprod(t(fa.ml_new$loadings)))
round(fa.ml_new$uniquenesses+diag(crossprod(t(fa.ml_new$loadings))),3)
```
Residuals
```{r}
S.hat_new<-L_new%*%t(L_new)+Psi_new
round(S.hat_new,3)
Residual_new<-S-S.hat_new
round(Residual_new,3)
res_new <- sum(Residual_new^2) 
res_new
```
The final error is pretty small 0.003930445 for m=3 with respect then the other for m=2, in fact the estimated correlation matrix is the same of the original ones. 

Proportion of variance 
```{r}
cum_new <- sum(diag(crossprod(fa.ml_new$loadings))/p*100)
prop_var_new <- round(c(diag(crossprod(fa.ml_new$loadings))/p*100, cum_new),1)
names(prop_var_new) <- c("Prop of var of Factor1", "Prop of var of Factor2", 
                     "Prop of var of Factor 3", "Cumulative prop of var")
prop_var_new
```

As before the first factor explain most of  the variance 48%, while the factor 2 explain another 37% and factor 3 only 8%, so the cumulative proportion of the variance explained, is 93% which is high, and this is not a positive thing because we might have an overfitting of the data. 

Now put everything together 
```{r}
#Factor quantities for m=2
output1 <-list( "factors"=fa.ml$loadings[,1:2], 
                "communalities"= diag(crossprod(t(fa.ml$loadings))),
                "uniquenesses" = fa.ml$uniquenesses, 
                "prop_of_variance"=round(cum, 3), 
                "sum_of_residual"=round(res,4))
output1

#Factor quantities for m=3
output2 <-list( "factors"=fa.ml_new$loadings[,1:3], 
                "communalities"= diag(crossprod(t(fa.ml_new$loadings))),
                "uniquenesses" = fa.ml_new$uniquenesses, 
                "prop_of_variance"=round(cum_new, 3), 
                "sum_of_residual"=round(res_new,4))
output2
```
How many factor may we choose?  
Since the goal is to minimize the number of factors, one idea could be to consider m = 2.
In fact, we observe that:


1. With m = 2, the proportion of total variance explained by the factors is 88%, whereas with m = 3 it is 93%. However, the third factor only explains an additional 8% of the variance, which might be negligible.


2. When we have 3 factors, the third one does not have high loading values, meaning its contribution to the variables is low—almost insignificant—so it does not add interpretability to the data.


3. As for the final error, it is 0.1065 for m = 2, while it drops to 0.0039 for m = 3, which is very small. This could indicate that the model fits the data too well and might lead to overfitting.


4. Regarding the specific variance of the variable FFF, in both cases it is not well explained by the model. The difference between the two is about 0.1, which is not significant in deciding the value of m.

Therefore, after making all these considerations, we choose m = 2 factors.

### Point 2 

Give an interpretation to the common factors in the m = 2 solution.
```{r}
fa.ml<-factanal(x=pulp_paper_scaled, factors=2, rotation = "varimax")
round(fa.ml$loadings[,],3)
```
Interpretation:  
As we know from the theory higher values for loadings means that higher is the contributions of the Factor to the variables, so factor 1 has very high value for BL, EM, SF, BS which are the variables related to the paper, so the factor 1 represent the strength of the paper.  
For factor 2 we have high values for AFL, LFF, FFF. All these properties are related to the lenght of the fibers; this second factor can be interpreted as fiber length.  
We notice that the variable ZST has moderately high loadings on both factors, indicating that it may be influenced by both the overall strength of the paper and the properties of the fibers. In fact, the greater the fiber length, the stronger the fibers tend to be (high value of ZST).    
FFF has a negative loading on Factor 2, reinforcing the idea that longer fibers (high AFL and LFF) are associated with fewer fine fibers (low FFF), which is consistent with fiber quality.

Overall, the factor structure aligns well with the expected grouping: paper properties on one side, and fiber morphology on the other.  

Also in order to have a better visualization of these factors we decide to visualize them in a plot
```{r}
var.names<-names(pulp_paper)
plot(Factor2~Factor1,data=fa.ml$loadings[,],type="n",
xlim=c(-0.6,1),ylim=c(-1,1), main="pulp paper data (ML)")
text(Factor2~Factor1,data=fa.ml$loadings[,],labels=var.names)
abline(h=0,v=0,lty=2)
```
The variables BL, EM, SF, BS are close to the first axis as they are represented by the First Factor, 
the variables AFL, LFF are closer to the second axis then to the first, and also if we plot FFF in absolute value we will find it in the first quadrant of the graph.  
In this way we can see clearly the subdivsion into two groups represented by the two factors. We notice also the position of ZST which is between the two groups (indicating that it is influenced by both as we said before) 

### Point 3

Make a scatterplot of the factor scores for m = 2 obtained by the regression method. Is their correlation
equal to zero? Should we expect so? Comment.
```{r}
fa.ml<-factanal(x=pulp_paper_scaled, factors=2, rotation="varimax", 
                scores="regression")
fa.ml$scores[1,]

plot(fa.ml$scores[,1],fa.ml$scores[,2],pch=16,
xlab="ML1",ylab="ML2",main="Factor scores distribtuion", col="skyblue2")
abline(a=0,b=0, col= "red", lty=2)
abline(a=1,b=0, col= "black", lty=2)
abline(a=-1,b=0, col= "black", lty=2)
```
From the plot, we observe that the factor scores are distributed differently along the two axes: along ML2 (the second factor), the scores are mostly concentrated in the range between -1 and 1, whereas along ML1 (the first factor), they are spread over a much wider range.  
This suggests that there is greater variability among the observations with respect to the first factor (associated with paper strength), while the second factor (associated with fiber length) shows more limited variability.  
From a theoretical perspective, we know that the factors are uncorrelated, so we expect the correlation between the factor scores to be zero.  
As a result from the plot, we can hypothesize that paper strength does not significantly depend on fiber length: even when strength varies greatly, fiber length remains relatively stable.   
SO the lack of correlation suggests that paper strength is influenced by other elements not directly related to fiber length.  
As we can see from the plot, not all values are clustered between -1 and 1 for ML2. In fact, we can observe some outliers, which are values that represent a much higher or lower score for ML2 compared to the average.  

```{r}
cor(fa.ml$scores[,1],fa.ml$scores[,2])
```
We observe that the correlation between the factor scores is very close to 0. In fact, based on the plot, we had already hypothesized that there was no strong correlation.

###Point 4

Suppose we have a new observation (15.5, 5.5, 2,-0.55, 0.6, 65,-5, 1.2). Calculate the corresponding
m = 2 factor scores and add this bivariate point to the plot in 3. How is it placed compared to the rest
of the n = 62 points? Could you tell without computing the factor scores? Comment.

Our goal is to estimate this information using the data previously obtained.  
As a first step, we standardize the new observation using the mean and variance vectors from the pulp_paper dataset.
```{r}
new_obs <- c(15.5, 5.5, 2, -0.55, 0.6, 65, -5, 1.2)
new_obs_mat <- matrix(new_obs, nrow = 1)
#mean vector
mu<- as.numeric(colMeans(pulp_paper))
#variance vector 
v <- as.numeric(diag(cov(pulp_paper)))
new_obs_scaled <- scale(new_obs_mat, center=mu, scale=sqrt(v)) 
#new observation rescaled 
new_obs_s <- new_obs_scaled[1,]
names(new_obs_s)<- c("BL" ,"EM" ,"SF" ,"BS","AFL", "LFF", "FFF", "ZST")
```
Now we calculate factor scores using the following formula: $\hat{f_i}=\hat{L}^TS^{-1}x$ where:

- $\hat{f_i}$ are the factor scores;
- $\hat{L}^T$ is the loadings matrix;
- $S^{-1}$ is the correlation matrix;
- $x$ is the scaled observation.

As we have seen in theory we use the correlation matrix $S$ and not the estimated one $\hat{L}\hat{L}^T+ \hat{\Psi}$ in order to reduce possibly incorrect determination of the number of factors. 

```{r}
#compute the factor score of the point 
lambda.inv <- solve(S) %*% fa.ml$loadings
point <- new_obs_s  %*% lambda.inv
point
```

```{r}
plot(fa.ml$scores[,1],fa.ml$scores[,2],pch=16,xlim = c(-5,5), ylim=c(-5,5),
xlab="ML1",ylab="ML2",main="Factor scores distribtuion", col="skyblue2")
points(point,col="red")
```
As we can see from the position of the factor scores in the plot, the new observation is an outlier. 

We can say that the new observation is an outlier even without computing the scores, in fact if we look to the mean of pulp paper and to our observation : 
```{r}
matrix <-rbind(mu, sqrt(v) , new_obs, new_obs_s)
rownames(matrix) <- c("pulp's mean", "pulp's standard deviation", "new obs", "new obs scaled")
colnames(matrix) <- c("BL" ,"EM" ,"SF" ,"BS","AFL", "LFF", "FFF", "ZST")
matrix
```

We can observe that the new observation shows values that differ significantly from the dataset's averages. 
For example, the LFF variable has a value of 65, while the dataset's mean is around 39. Similarly, FFF has a value of -5, whereas its mean is approximately 26.67. This suggests that the new observation deviates substantially from the data used in our analysis. Moreover, since FFF represents a percentage, it is mathematically impossible for it to take a negative value.
Looking to ZST for the rescaled new obs we can see that this observation corresponds to a z-score of approximately 4.52. This indicates that the new value lies more than 4.5 standard deviations above the mean.   
This strongly indicates that the data point is anomalous, possibly due to a measurement error.